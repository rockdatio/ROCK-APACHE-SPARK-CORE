from pyspark.sql import SparkSession

if __name__ == '__main__':
    spark = SparkSession.builder.appName('TIME').getOrCreate()
    spark.conf.set("spark.executor.memory", '8g')
    spark.conf.set("spark.executor.instances", "4")
    spark.conf.set('spark.executor.cores', '3')
    spark.conf.set('spark.cores.max', '3')
    spark.conf.set("spark.driver.memory", '8g')

    # .set("spark.sql.sources.partitionOverwriteMode", "dynamic")
    # .set("spark.app.name", configParam("spark.app.name"))
    # .set("spark.debug.maxToStringFields", configParam("spark.debug.maxToStringFields"))
    # .set("spark.default.parallelism", configParam("spark.default.parallelism"))
    # .set("spark.driver.cores", configParam("spark.driver.cores"))
    # .set("spark.driver.memory", configParam("spark.driver.memory"))
    # .set("spark.driver.memoryOverhead", configParam("spark.driver.memoryOverhead"))
    # .set("spark.dynamicAllocation.enabled", configParam("spark.dynamicAllocation.enabled"))
    # .set("spark.dynamicAllocation.maxExecutors", configParam("spark.dynamicAllocation.maxExecutors"))
    # .set("spark.executor.cores", configParam("spark.executor.cores"))
    # .set("spark.executor.instances", configParam("spark.executor.instances"))
    # .set("spark.executor.memory", configParam("spark.executor.memory"))
    # .set("spark.executor.memoryOverhead", configParam("spark.executor.memoryOverhead"))
    # .set("spark.hadoop.hive.exec.compress.output", configParam("spark.hadoop.hive.exec.compress.output"))
    # .set("spark.hadoop.hive.exec.dynamic.partition", configParam("spark.hadoop.hive.exec.dynamic.partition"))
    # .set("spark.hadoop.hive.exec.dynamic.partition.mode", configParam("spark.hadoop.hive.exec.dynamic.partition.mode"))
    # .set("spark.kryoserializer.buffer.max", configParam("spark.kryoserializer.buffer.max"))
    # .set("spark.master", configParam("spark.master"))
    # .set("spark.shuffle.service.enabled", configParam("spark.shuffle.service.enabled"))
    # .set("spark.sql.autoBroadcastJoinThreshold", configParam("spark.sql.autoBroadcastJoinThreshold"))
    # .set("spark.sql.crossJoin.enabled", configParam("spark.sql.crossJoin.enabled"))
    # .set("spark.sql.join.preferSortMergeJoin", configParam("spark.sql.join.preferSortMergeJoin"))
    # .set("spark.sql.parquet.writeLegacyFormat", configParam("spark.sql.parquet.writeLegacyFormat"))
    # .set("spark.sql.shuffle.partitions", configParam("spark.sql.shuffle.partitions"))
    # .set("spark.submit.deployMode", configParam("spark.submit.deployMode"))
    # .set("spark.ui.enabled", configParam("spark.ui.enabled"))
    # .set("sparkSubmit.callIncludeProcessName", configParam("sparkSubmit.callIncludeProcessName"))
    # .set("sparkSubmit.class", configParam("sparkSubmit.class"))
    # .set("sparkSubmit.jars", configParam("sparkSubmit.jars"))
    # .set("sparkSubmit.jars", configParam("sparkSubmit.jars"))
    sc = spark.sparkContext

    spark.stop()
